{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":11435964,"sourceType":"datasetVersion","datasetId":7163131}],"dockerImageVersionId":31012,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-04-17T06:13:15.764430Z","iopub.execute_input":"2025-04-17T06:13:15.764755Z","iopub.status.idle":"2025-04-17T06:13:15.780961Z","shell.execute_reply.started":"2025-04-17T06:13:15.764733Z","shell.execute_reply":"2025-04-17T06:13:15.779664Z"}},"outputs":[{"name":"stdout","text":"/kaggle/input/fuladata/fulbe_adlam.txt\n/kaggle/input/fuladata/Sustainability.txt\n/kaggle/input/fuladata/binndi_adlam.txt\n/kaggle/input/fuladata/peace_adlam.txt\n/kaggle/input/fuladata/Sustainability_adlam.txt\n/kaggle/input/fuladata/peace.txt\n/kaggle/input/fuladata/pulaar_adlam.txt\n/kaggle/input/fuladata/tech_adlam.txt\n/kaggle/input/fuladata/teddungal_latin.txt\n/kaggle/input/fuladata/teddungal_adlam.txt\n/kaggle/input/fuladata/fulbe.txt\n/kaggle/input/fuladata/tech.txt\n/kaggle/input/fuladata/binndi_latin.txt\n/kaggle/input/fuladata/pulaar_latin.txt\n","output_type":"stream"}],"execution_count":40},{"cell_type":"code","source":"rm -rf /kaggle/working/adlam_texts_fixed\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-17T05:36:04.768675Z","iopub.execute_input":"2025-04-17T05:36:04.769059Z","iopub.status.idle":"2025-04-17T05:36:04.907780Z","shell.execute_reply.started":"2025-04-17T05:36:04.769028Z","shell.execute_reply":"2025-04-17T05:36:04.906385Z"}},"outputs":[],"execution_count":26},{"cell_type":"code","source":"import os\nimport re # Keep re imported, might be useful for future cleaning steps\n\n# --- Configuration ---\n# Path to your folder with original .txt files\ninput_folder = \"/kaggle/input/fuladata\"\n# Path where processed individual files will be saved\noutput_folder = \"/kaggle/working/adlam\"\n# Name for the final combined file\ncombined_filename = \"combined_adlam_output.txt\"\n# Path for the final combined file\ncombined_output_path = os.path.join(output_folder, combined_filename)\n\n# --- RTL Marker ---\n# Unicode marker for Right-to-Left scripts (important for proper display)\nrtl_marker = \"\\u200F\"\n\n# --- Helper Functions ---\n\ndef is_adlam_file(filename):\n    \"\"\"Check if a file is an Adlam text file based on its suffix.\"\"\"\n    return filename.endswith(\"_adlam.txt\")\n\ndef clean_and_format_rtl_text(lines):\n    \"\"\"\n    Cleans lines, combines them into paragraphs, and formats for RTL display.\n\n    Args:\n        lines (list): A list of strings, where each string is a line from the input file.\n\n    Returns:\n        list: A list of strings, where each string is a processed paragraph\n              prepended with the RTL marker and ending with a newline.\n    \"\"\"\n    cleaned_paragraphs = []\n    current_paragraph = \"\"\n    for line in lines:\n        stripped_line = line.strip()\n        if stripped_line:\n            # If the line has content, add it to the current paragraph\n            # Add a space separator if the paragraph already has content\n            if current_paragraph:\n                current_paragraph += \" \" + stripped_line\n            else:\n                current_paragraph = stripped_line\n        else:\n            # If the line is empty and we have content in current_paragraph,\n            # it marks the end of a paragraph.\n            if current_paragraph:\n                # Add RTL marker at the beginning and a newline at the end\n                cleaned_paragraphs.append(rtl_marker + current_paragraph + \"\\n\")\n                current_paragraph = \"\" # Reset for the next paragraph\n\n    # Add the last paragraph if the file doesn't end with a blank line\n    if current_paragraph:\n        cleaned_paragraphs.append(rtl_marker + current_paragraph + \"\\n\")\n\n    return cleaned_paragraphs\n\ndef process_adlam_file(input_path, output_path):\n    \"\"\"\n    Reads an Adlam file, cleans/formats it for RTL, and saves the result.\n\n    Args:\n        input_path (str): Path to the input Adlam text file.\n        output_path (str): Path where the processed file should be saved.\n    \"\"\"\n    try:\n        with open(input_path, \"r\", encoding=\"utf-8\") as f_in:\n            lines = f_in.readlines()\n\n        processed_paragraphs = clean_and_format_rtl_text(lines)\n\n        # Ensure the output directory exists (though created earlier, good practice)\n        os.makedirs(os.path.dirname(output_path), exist_ok=True)\n\n        with open(output_path, \"w\", encoding=\"utf-8\") as f_out:\n            f_out.writelines(processed_paragraphs)\n\n        print(f\"âœ… Processed and saved RTL formatted: {os.path.basename(output_path)}\")\n        return True # Indicate success\n\n    except FileNotFoundError:\n        print(f\"âŒ Error: Input file not found: {input_path}\")\n        return False\n    except Exception as e:\n        print(f\"âŒ Error processing file {input_path}: {e}\")\n        return False\n\n# --- Main Processing Logic ---\n\n# 1. Create the output directory if it doesn't exist\nos.makedirs(output_folder, exist_ok=True)\nprint(f\"Output directory set to: {output_folder}\")\n\nprocessed_files_list = []\n\n# 2. Process each Adlam file individually\nprint(\"\\n--- Processing Individual Files ---\")\nif not os.path.exists(input_folder):\n     print(f\"âŒ Error: Input folder not found: {input_folder}\")\nelse:\n    for filename in os.listdir(input_folder):\n        input_path = os.path.join(input_folder, filename)\n        if os.path.isfile(input_path) and is_adlam_file(filename):\n            output_path = os.path.join(output_folder, filename)\n            if process_adlam_file(input_path, output_path):\n                 processed_files_list.append(output_path) # Keep track of successfully processed files\n        elif os.path.isfile(input_path):\n            print(f\"â© Skipped (not an Adlam file): {filename}\")\n        # Optional: handle directories if needed, otherwise ignore them\n\n# 3. Combine processed files into a single file\nprint(\"\\n--- Combining Processed Files ---\")\nif not processed_files_list:\n    print(\"â“ No Adlam files were processed. Cannot create combined file.\")\nelse:\n    try:\n        with open(combined_output_path, \"w\", encoding=\"utf-8\") as f_combined:\n            print(f\"ğŸ“ Creating combined file: {combined_output_path}\")\n            for file_path in processed_files_list:\n                try:\n                    with open(file_path, \"r\", encoding=\"utf-8\") as f_processed:\n                        content = f_processed.read()\n                        f_combined.write(content)\n                        # Add an extra newline between content of different files\n                        # This helps separate the text from different original files\n                        f_combined.write(\"\\n\")\n                    print(f\"  + Added content from: {os.path.basename(file_path)}\")\n                except Exception as e:\n                     print(f\"  âŒ Error reading processed file {os.path.basename(file_path)}: {e}. Skipping this file.\")\n\n        print(f\"ğŸ‰ Successfully combined {len(processed_files_list)} processed files into: {combined_filename}\")\n\n    except Exception as e:\n        print(f\"âŒ Error creating combined file {combined_output_path}: {e}\")\n\nprint(\"\\n--- Script Finished ---\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-17T06:00:44.995292Z","iopub.execute_input":"2025-04-17T06:00:44.995674Z","iopub.status.idle":"2025-04-17T06:00:45.775856Z","shell.execute_reply.started":"2025-04-17T06:00:44.995651Z","shell.execute_reply":"2025-04-17T06:00:45.774577Z"}},"outputs":[{"name":"stdout","text":"Output directory set to: /kaggle/working/adlam\n\n--- Processing Individual Files ---\nâœ… Processed and saved RTL formatted: fulbe_adlam.txt\nâ© Skipped (not an Adlam file): Sustainability.txt\nâœ… Processed and saved RTL formatted: binndi_adlam.txt\nâœ… Processed and saved RTL formatted: peace_adlam.txt\nâœ… Processed and saved RTL formatted: Sustainability_adlam.txt\nâ© Skipped (not an Adlam file): peace.txt\nâœ… Processed and saved RTL formatted: pulaar_adlam.txt\nâœ… Processed and saved RTL formatted: tech_adlam.txt\nâ© Skipped (not an Adlam file): teddungal_latin.txt\nâœ… Processed and saved RTL formatted: teddungal_adlam.txt\nâ© Skipped (not an Adlam file): fulbe.txt\nâ© Skipped (not an Adlam file): tech.txt\nâ© Skipped (not an Adlam file): binndi_latin.txt\nâ© Skipped (not an Adlam file): pulaar_latin.txt\n\n--- Combining Processed Files ---\nğŸ“ Creating combined file: /kaggle/working/adlam/combined_adlam_output.txt\n  + Added content from: fulbe_adlam.txt\n  + Added content from: binndi_adlam.txt\n  + Added content from: peace_adlam.txt\n  + Added content from: Sustainability_adlam.txt\n  + Added content from: pulaar_adlam.txt\n  + Added content from: tech_adlam.txt\n  + Added content from: teddungal_adlam.txt\nğŸ‰ Successfully combined 7 processed files into: combined_adlam_output.txt\n\n--- Script Finished ---\n","output_type":"stream"}],"execution_count":36},{"cell_type":"code","source":"\nwith open(\"/kaggle/working/adlam/combined_adlam_output.txt\", \"r\", encoding=\"utf-8\") as f:\n            adlam = f.read()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-17T06:36:29.455272Z","iopub.execute_input":"2025-04-17T06:36:29.456546Z","iopub.status.idle":"2025-04-17T06:36:29.821571Z","shell.execute_reply.started":"2025-04-17T06:36:29.456491Z","shell.execute_reply":"2025-04-17T06:36:29.820629Z"}},"outputs":[],"execution_count":43},{"cell_type":"code","source":"! pip install fire","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-17T07:19:35.791843Z","iopub.execute_input":"2025-04-17T07:19:35.792842Z","iopub.status.idle":"2025-04-17T07:19:42.260396Z","shell.execute_reply.started":"2025-04-17T07:19:35.792815Z","shell.execute_reply":"2025-04-17T07:19:42.259082Z"}},"outputs":[{"name":"stderr","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"},{"name":"stdout","text":"Collecting fire\n  Downloading fire-0.7.0.tar.gz (87 kB)\n\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m87.2/87.2 kB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\nRequirement already satisfied: termcolor in /usr/local/lib/python3.11/dist-packages (from fire) (2.5.0)\nBuilding wheels for collected packages: fire\n  Building wheel for fire (setup.py) ... \u001b[?25l\u001b[?25hdone\n  Created wheel for fire: filename=fire-0.7.0-py3-none-any.whl size=114249 sha256=9146311f96ca21a2534a06b5688ddf4fdc558098604a58f7681b3fe7fd2b7d24\n  Stored in directory: /root/.cache/pip/wheels/46/54/24/1624fd5b8674eb1188623f7e8e17cdf7c0f6c24b609dfb8a89\nSuccessfully built fire\nInstalling collected packages: fire\nSuccessfully installed fire-0.7.0\n","output_type":"stream"}],"execution_count":61},{"cell_type":"code","source":"# adlam_tokenizer_training.py\n\nimport fire\nimport os\nimport sentencepiece as spm\n\ndef train_tokenizer(\n    data_file: str,\n    save_path: str,\n    vocab_size: int = 16000,\n    num_threads: int = 8\n):\n    \"\"\"\n    Train a SentencePiece BPE tokenizer on Adlam Fula data.\n\n    Args:\n        data_file (str): Path to the combined text file.\n        save_path (str): Directory where the tokenizer model will be saved.\n        vocab_size (int, optional): Size of the tokenizer vocabulary. Defaults to 16000.\n        num_threads (int, optional): Number of CPU threads to use. Defaults to 8.\n    \"\"\"\n    os.makedirs(save_path, exist_ok=True)\n    tokenizer_name = os.path.join(save_path, \"tokenizer\")\n\n    spm.SentencePieceTrainer.train(\n        input=data_file,\n        model_prefix=tokenizer_name,\n        vocab_size=vocab_size,\n        num_threads=num_threads,\n        model_type=\"bpe\",\n        max_sentence_length=1073741824,\n        shuffle_input_sentence=True,\n        character_coverage=1.0,\n        hard_vocab_limit=False,\n    )\n\n \n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-17T07:32:17.598155Z","iopub.execute_input":"2025-04-17T07:32:17.598505Z","iopub.status.idle":"2025-04-17T07:32:17.605306Z","shell.execute_reply.started":"2025-04-17T07:32:17.598451Z","shell.execute_reply":"2025-04-17T07:32:17.604381Z"}},"outputs":[],"execution_count":68},{"cell_type":"code","source":"data_file = \"/kaggle/working/adlam/combined_adlam_output.txt\"\noutput = \"/kaggle/working/adlam_tokenizer\"\ntrain_tokenizer(data_file,output)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-17T07:32:37.388022Z","iopub.execute_input":"2025-04-17T07:32:37.388382Z","iopub.status.idle":"2025-04-17T07:32:53.565812Z","shell.execute_reply.started":"2025-04-17T07:32:37.388356Z","shell.execute_reply":"2025-04-17T07:32:53.564976Z"}},"outputs":[],"execution_count":69},{"cell_type":"code","source":"from transformers import PreTrainedTokenizerFast\n\ndef load_adlam_tokenizer():\n    \"\"\"\n    Load the trained Adlam tokenizer from the specified path.\n\n    Args:\n        tokenizer_path (str): Path to the SentencePiece `.model` file (without extension if using model prefix).\n    Returns:\n        PreTrainedTokenizerFast: Hugging Face-compatible tokenizer\n    \"\"\"\n    tokenizer = PreTrainedTokenizerFast(\n        tokenizer_file=\"/kaggle/working/adlam_fula_tokenizer/tokenizer.json\",  # SentencePiece model\n        unk_token=\"<unk>\",\n        pad_token=\"<pad>\",\n        bos_token=\"<s>\",\n        eos_token=\"</s>\",\n    )\n\n    print(\"âœ… Tokenizer loaded successfully!\")\n    return tokenizer\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-17T07:50:39.930404Z","iopub.execute_input":"2025-04-17T07:50:39.930755Z","iopub.status.idle":"2025-04-17T07:50:39.936679Z","shell.execute_reply.started":"2025-04-17T07:50:39.930734Z","shell.execute_reply":"2025-04-17T07:50:39.935338Z"}},"outputs":[],"execution_count":86},{"cell_type":"code","source":"from transformers import PreTrainedTokenizerFast\n\ntokenizer = PreTrainedTokenizerFast(tokenizer_file=\"/kaggle/working/adlam_fula_tokenizer/tokenizer.json\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-17T07:56:22.926448Z","iopub.execute_input":"2025-04-17T07:56:22.926852Z","iopub.status.idle":"2025-04-17T07:56:23.173194Z","shell.execute_reply.started":"2025-04-17T07:56:22.926828Z","shell.execute_reply":"2025-04-17T07:56:23.172071Z"}},"outputs":[],"execution_count":92},{"cell_type":"code","source":"# ğŸ” Test a sample Adlam sentence\n#sample = \"ğ¤²ğ¥‹ğ¤¶ğ¤«ğ¤¤ ğ¤¢ğ¤§ğ¤¼ğ¤« ğ¤©ğ¤® ğ¤²ğ¤¢ğ¥„ğ¤ªğ¤¢ğ¤¤\"\nprint(\"Tokens:\", tokenizer.tokenize(sample))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-17T07:57:29.580532Z","iopub.execute_input":"2025-04-17T07:57:29.580838Z","iopub.status.idle":"2025-04-17T07:57:29.587367Z","shell.execute_reply.started":"2025-04-17T07:57:29.580819Z","shell.execute_reply":"2025-04-17T07:57:29.586194Z"}},"outputs":[{"name":"stdout","text":"Tokens: ['ğ¤‰', 'ğ¤ºğ¤­ğ¤¤ğ¤¢', 'ğ¤²', \"'\", 'ğ¤£ğ¤«ğ¤²', '[UNK]', 'ğ¤£ğ¤¢ğ¤ªğ¤¼ğ¤®ğ¤¤', 'ğ¤§ğ¤µğ¤³ğ¤¢', 'ğ¤£ğ¤«ğ¤¦ğ¤®', 'ğ¤®', 'ğ¤«', 'ğ¤²ğ¤¢ğ¤ºğ¤«', 'ğ¤¥ğ¤µğ¤¥', 'ğ¤¤ğ¤«ğ¤±ğ¤ªğ¤µ', 'ğ¤­ğ¤²ğ¤¢', 'ğ¤´ğ¤¢ğ¤¸ğ¤ªğ¤¢', 'ğ¤«', 'ğ¤£ğ¤µğ¤©ğ¤­', 'ğ¥‘ğ¥˜', '[UNK]', 'ğ¤­ğ¤²ğ¤¢', 'ğ¤²', \"'\", 'ğ¤¶ğ¤µğ¤©ğ¤­ğ¤²ğ¤«', 'ğ¤«', 'ğ¤¶ğ¤­ğ¤¥ğ¤¯ğ¤­', 'ğ¤¬ğ¤µğ¤¤ğ¤©ğ¤«', 'ğ¤¶ğ¤­ğ¤¥ğ¤¯ğ¤­', 'ğ¤¤ğ¤«ğ¤§', 'ğ¤¢ğ¤§ğ¤¢ğ¤¥ğ¤¢ğ¤²', 'ğ¤³ğ¤®ğ¤£ğ¤«', '.']\n","output_type":"stream"}],"execution_count":95},{"cell_type":"code","source":"# Example usage\nadlam_tokenizer = load_adlam_tokenizer()\n\n# ğŸ” Test a sample Adlam sentence\nsample = \"ğ¤‰ ğ¤ºğ¤­ğ¤¤ğ¤¢ ğ¤²'ğ¤£ğ¤«ğ¥…ğ¤², ğ¤£ğ¤¢ğ¥„ğ¤ªğ¤¼ğ¤®ğ¤¤ ğ¤§ğ¤µğ¤³ğ¤¢ ğ¤£ğ¤«ğ¤¦ğ¥†ğ¤® ğ¤®ğ¥… ğ¤« ğ¤²ğ¤¢ğ¤ºğ¥†ğ¤« ğ¤¥ğ¤µğ¤¥ ğ¤¤ğ¤«ğ¤±ğ¤ªğ¤µ ğ¤­ğ¤²ğ¤¢ ğ¤´ğ¤¢ğ¤¸ğ¤ªğ¤¢ ğ¤« ğ¤£ğ¤µğ¥…ğ¤©ğ¤­ ğ¥‘ğ¥˜, ğ¤­ğ¤²ğ¤¢ ğ¤²'ğ¤¶ğ¤µğ¤©ğ¥†ğ¤­ğ¤²ğ¤«ğ¥… ğ¤« ğ¤¶ğ¤­ğ¤¥ğ¤¯ğ¤­ ğ¤¬ğ¤µğ¤¤ğ¤©ğ¤« ğ¤¶ğ¤­ğ¤¥ğ¤¯ğ¤­ ğ¤¤ğ¤«ğ¤§ ğ¤¢ğ¤§ğ¤¢ğ¤¥ğ¤¢ğ¥„ğ¤² ğ¤³ğ¤®ğ¥…ğ¤£ğ¤«.\"\nprint(\"Tokens:\", adlam_tokenizer.tokenize(sample))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-17T07:57:22.792097Z","iopub.execute_input":"2025-04-17T07:57:22.792489Z","iopub.status.idle":"2025-04-17T07:57:23.072212Z","shell.execute_reply.started":"2025-04-17T07:57:22.792446Z","shell.execute_reply":"2025-04-17T07:57:23.071170Z"}},"outputs":[{"name":"stdout","text":"âœ… Tokenizer loaded successfully!\nTokens: ['ğ¤‰', 'ğ¤ºğ¤­ğ¤¤ğ¤¢', 'ğ¤²', \"'\", 'ğ¤£ğ¤«ğ¤²', '[UNK]', 'ğ¤£ğ¤¢ğ¤ªğ¤¼ğ¤®ğ¤¤', 'ğ¤§ğ¤µğ¤³ğ¤¢', 'ğ¤£ğ¤«ğ¤¦ğ¤®', 'ğ¤®', 'ğ¤«', 'ğ¤²ğ¤¢ğ¤ºğ¤«', 'ğ¤¥ğ¤µğ¤¥', 'ğ¤¤ğ¤«ğ¤±ğ¤ªğ¤µ', 'ğ¤­ğ¤²ğ¤¢', 'ğ¤´ğ¤¢ğ¤¸ğ¤ªğ¤¢', 'ğ¤«', 'ğ¤£ğ¤µğ¤©ğ¤­', 'ğ¥‘ğ¥˜', '[UNK]', 'ğ¤­ğ¤²ğ¤¢', 'ğ¤²', \"'\", 'ğ¤¶ğ¤µğ¤©ğ¤­ğ¤²ğ¤«', 'ğ¤«', 'ğ¤¶ğ¤­ğ¤¥ğ¤¯ğ¤­', 'ğ¤¬ğ¤µğ¤¤ğ¤©ğ¤«', 'ğ¤¶ğ¤­ğ¤¥ğ¤¯ğ¤­', 'ğ¤¤ğ¤«ğ¤§', 'ğ¤¢ğ¤§ğ¤¢ğ¤¥ğ¤¢ğ¤²', 'ğ¤³ğ¤®ğ¤£ğ¤«', '.']\n","output_type":"stream"}],"execution_count":94},{"cell_type":"code","source":"from transformers import AutoTokenizer\n\ntokenizer = AutoTokenizer.from_pretrained(\"/kaggle/working/adlam_tokenizer\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#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\nlogin(token=\" \")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-17T08:05:39.105950Z","iopub.execute_input":"2025-04-17T08:05:39.106344Z","iopub.status.idle":"2025-04-17T08:05:39.211800Z","shell.execute_reply.started":"2025-04-17T08:05:39.106320Z","shell.execute_reply":"2025-04-17T08:05:39.210980Z"}},"outputs":[],"execution_count":97},{"cell_type":"code","source":"from transformers import AutoTokenizer, PreTrainedTokenizerFast\n\n# 1. Load LLaMA tokenizer\nllama_tokeniser = AutoTokenizer.from_pretrained(\"cawoylel/firtanam\") \n# Replace with LLaMA model name\n\n# 2. Load Adlam tokenizer (make sure itâ€™s in the Hugging Face-compatible format, such as tokenizer.json)\nadlam_tokenizer = PreTrainedTokenizerFast.from_pretrained(\"/kaggle/working/adlam_fula_tokenizer\")\n\n# 3. Check current vocab size\nprint(f\"Current LLaMA tokenizer vocab size: {len(llama_tokenizer)}\")\nprint(f\"Current Adlam tokenizer vocab size: {len(adlam_tokenizer)}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-17T08:26:16.033666Z","iopub.execute_input":"2025-04-17T08:26:16.034006Z","iopub.status.idle":"2025-04-17T08:26:17.190735Z","shell.execute_reply.started":"2025-04-17T08:26:16.033983Z","shell.execute_reply":"2025-04-17T08:26:17.189737Z"}},"outputs":[{"name":"stdout","text":"Current LLaMA tokenizer vocab size: 160155\nCurrent Adlam tokenizer vocab size: 32000\n","output_type":"stream"}],"execution_count":118},{"cell_type":"code","source":"# Test with some sample text in Adlam and LLaMA\nsample_adlam_text = \"ğ¤¢ ğ¤²'ğ¤¶ğ¤µğ¤©ğ¥†ğ¤­ğ¤²ğ¤«ğ¥… ğ¤« ğ¤¶ğ¤­ğ¤¥ğ¤¯ğ¤­ ğ¤¬ğ¤µğ¤¤ğ¤©ğ¤« ğ¤¶ğ¤­ğ¤¥ğ¤¯ğ¤­ ğ¤¤ğ¤«ğ¤§ ğ¤¢ğ¤§ğ¤¢ğ¤¥ğ¤¢ğ¥„ğ¤² ğ¤³ğ¤®ğ¥…ğ¤£ğ¤«.ğ¤‘ğ¤®ğ¤²ğ¤® ğ¤´ğ¤®ğ¤²ğ¤¼ğ¤¢ğ¥„ğ¤¶ğ¤­ ğ¤´ğ¤®ğ¥…ğ¤ªğ¤® ğ¤ºğ¤®ğ¥…ğ¤¼ğ¤®, ğ¤¥ğ¤¢ğ¥„ğ¤´ğ¤®ğ¥…ğ¤¶ğ¤­ ğ¤¯ğ¤­ğ¥… ğ¤©ğ¤µğ¥…ğ¤©ğ¤¼ğ¤­ğ¥…, ğ¤¸ğ¤µğ¤¯ğ¤® ğ¤©ğ¤µğ¥…ğ¤©ğ¤¼ğ¤­ğ¥….\"\ntokenized_adlam = llama_tokeniser(sample_adlam_text)\nprint(\"Tokenized Adlam Text:\", tokenized_adlam)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-17T08:27:20.269175Z","iopub.execute_input":"2025-04-17T08:27:20.269508Z","iopub.status.idle":"2025-04-17T08:27:20.276589Z","shell.execute_reply.started":"2025-04-17T08:27:20.269487Z","shell.execute_reply":"2025-04-17T08:27:20.275543Z"}},"outputs":[{"name":"stdout","text":"Tokenized Adlam Text: {'input_ids': [128000, 172, 252, 97, 95, 109697, 252, 97, 110, 6, 172, 252, 97, 114, 172, 252, 97, 113, 172, 252, 97, 102, 172, 252, 98, 228, 172, 252, 97, 255, 172, 252, 97, 110, 172, 252, 97, 104, 172, 252, 98, 227, 109697, 252, 97, 104, 109697, 252, 97, 114, 172, 252, 97, 255, 172, 252, 97, 98, 172, 252, 97, 107, 172, 252, 97, 255, 109697, 252, 97, 105, 172, 252, 97, 113, 172, 252, 97, 97, 172, 252, 97, 102, 172, 252, 97, 104, 109697, 252, 97, 114, 172, 252, 97, 255, 172, 252, 97, 98, 172, 252, 97, 107, 172, 252, 97, 255, 109697, 252, 97, 97, 172, 252, 97, 104, 172, 252, 97, 100, 109697, 252, 97, 95, 172, 252, 97, 100, 172, 252, 97, 95, 172, 252, 97, 98, 172, 252, 97, 95, 172, 252, 98, 226, 172, 252, 97, 110, 109697, 252, 97, 111, 172, 252, 97, 106, 172, 252, 98, 227, 172, 252, 97, 96, 172, 252, 97, 104, 13, 172, 252, 100701, 172, 252, 97, 106, 172, 252, 97, 110, 172, 252, 97, 106, 109697, 252, 97, 112, 172, 252, 97, 106, 172, 252, 97, 110, 172, 252, 97, 120, 172, 252, 97, 95, 172, 252, 98, 226, 172, 252, 97, 114, 172, 252, 97, 255, 109697, 252, 97, 112, 172, 252, 97, 106, 172, 252, 98, 227, 172, 252, 97, 103, 172, 252, 97, 106, 109697, 252, 97, 118, 172, 252, 97, 106, 172, 252, 98, 227, 172, 252, 97, 120, 172, 252, 97, 106, 11, 109697, 252, 97, 98, 172, 252, 97, 95, 172, 252, 98, 226, 172, 252, 97, 112, 172, 252, 97, 106, 172, 252, 98, 227, 172, 252, 97, 114, 172, 252, 97, 255, 109697, 252, 97, 107, 172, 252, 97, 255, 172, 252, 98, 227, 109697, 252, 97, 102, 172, 252, 97, 113, 172, 252, 98, 227, 172, 252, 97, 102, 172, 252, 97, 120, 172, 252, 97, 255, 172, 252, 98, 227, 11, 109697, 252, 97, 116, 172, 252, 97, 113, 172, 252, 97, 107, 172, 252, 97, 106, 109697, 252, 97, 102, 172, 252, 97, 113, 172, 252, 98, 227, 172, 252, 97, 102, 172, 252, 97, 120, 172, 252, 97, 255, 172, 252, 98, 227, 13], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n","output_type":"stream"}],"execution_count":119},{"cell_type":"code","source":"# Get vocabularies from both tokenizers\nllama_vocab = llama_tokenizer.get_vocab()\nadlam_vocab = adlam_tokenizer.get_vocab()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-17T08:15:19.080042Z","iopub.execute_input":"2025-04-17T08:15:19.080982Z","iopub.status.idle":"2025-04-17T08:15:19.279021Z","shell.execute_reply.started":"2025-04-17T08:15:19.080949Z","shell.execute_reply":"2025-04-17T08:15:19.277853Z"}},"outputs":[],"execution_count":104},{"cell_type":"code","source":"# Merge vocabularies by adding all Adlam tokens to LLaMA's vocabulary\ncombined_vocab = llama_vocab.copy()\ncombined_vocab.update(adlam_vocab)  # Add all Adlam tokens","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-17T08:16:37.018018Z","iopub.execute_input":"2025-04-17T08:16:37.018391Z","iopub.status.idle":"2025-04-17T08:16:37.038486Z","shell.execute_reply.started":"2025-04-17T08:16:37.018367Z","shell.execute_reply":"2025-04-17T08:16:37.037335Z"}},"outputs":[],"execution_count":107},{"cell_type":"code","source":"# Update LLaMA tokenizer with the combined vocab\nllama_tokenizer.add_tokens(list(adlam_vocab.keys()))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-17T08:16:55.875800Z","iopub.execute_input":"2025-04-17T08:16:55.876187Z","iopub.status.idle":"2025-04-17T08:17:01.596240Z","shell.execute_reply.started":"2025-04-17T08:16:55.876164Z","shell.execute_reply":"2025-04-17T08:17:01.595357Z"}},"outputs":[{"execution_count":108,"output_type":"execute_result","data":{"text/plain":"32000"},"metadata":{}}],"execution_count":108},{"cell_type":"code","source":"# Add special tokens if necessary\nspecial_tokens = {\n    'unk_token': '<unk>',\n    'pad_token': '<pad>',\n    'bos_token': '<bos>',\n    'eos_token': '<eos>',\n}\n\nllama_tokenizer.add_special_tokens(special_tokens)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-17T08:17:16.545428Z","iopub.execute_input":"2025-04-17T08:17:16.546485Z","iopub.status.idle":"2025-04-17T08:17:16.842470Z","shell.execute_reply.started":"2025-04-17T08:17:16.546421Z","shell.execute_reply":"2025-04-17T08:17:16.841666Z"}},"outputs":[{"execution_count":109,"output_type":"execute_result","data":{"text/plain":"4"},"metadata":{}}],"execution_count":109},{"cell_type":"code","source":"# Save the extended tokenizer\noutput_dir = \"/kaggle/working/extended_llama_adlam_tokenizer\"\nllama_tokenizer.save_pretrained(output_dir)\n\nprint(\"Extended LLaMA tokenizer with all Adlam tokens saved at:\", output_dir)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-17T08:17:32.087269Z","iopub.execute_input":"2025-04-17T08:17:32.087667Z","iopub.status.idle":"2025-04-17T08:17:33.265007Z","shell.execute_reply.started":"2025-04-17T08:17:32.087644Z","shell.execute_reply":"2025-04-17T08:17:33.264024Z"}},"outputs":[{"name":"stdout","text":"Extended LLaMA tokenizer with all Adlam tokens saved at: /kaggle/working/extended_llama_adlam_tokenizer\n","output_type":"stream"}],"execution_count":110},{"cell_type":"code","source":"# Test with some sample text in Adlam and LLaMA\nsample_adlam_text = \"ğ¤¢ ğ¤²'ğ¤¶ğ¤µğ¤©ğ¥†ğ¤­ğ¤²ğ¤«ğ¥… ğ¤« ğ¤¶ğ¤­ğ¤¥ğ¤¯ğ¤­ ğ¤¬ğ¤µğ¤¤ğ¤©ğ¤« ğ¤¶ğ¤­ğ¤¥ğ¤¯ğ¤­ ğ¤¤ğ¤«ğ¤§ ğ¤¢ğ¤§ğ¤¢ğ¤¥ğ¤¢ğ¥„ğ¤² ğ¤³ğ¤®ğ¥…ğ¤£ğ¤«.ğ¤‘ğ¤®ğ¤²ğ¤® ğ¤´ğ¤®ğ¤²ğ¤¼ğ¤¢ğ¥„ğ¤¶ğ¤­ ğ¤´ğ¤®ğ¥…ğ¤ªğ¤® ğ¤ºğ¤®ğ¥…ğ¤¼ğ¤®, ğ¤¥ğ¤¢ğ¥„ğ¤´ğ¤®ğ¥…ğ¤¶ğ¤­ ğ¤¯ğ¤­ğ¥… ğ¤©ğ¤µğ¥…ğ¤©ğ¤¼ğ¤­ğ¥…, ğ¤¸ğ¤µğ¤¯ğ¤® ğ¤©ğ¤µğ¥…ğ¤©ğ¤¼ğ¤­ğ¥….\"\ntokenized_adlam = llama_tokenizer(sample_adlam_text)\nprint(\"Tokenized Adlam Text:\", tokenized_adlam)\n\nsample_llama_text = \"E gila ndeen, daartol suka debbo oo e nagge mum lewru ina yahra e duuÉ“i 18, ina njuÉ“É“inee e jimÉ—i fulÉ“e jimÉ—i les asamaan koode.\"\ntokenized_llama = llama_tokenizer(sample_llama_text)\nprint(\"Tokenized LLaMA Text:\", tokenized_llama)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-17T08:22:55.907751Z","iopub.execute_input":"2025-04-17T08:22:55.908141Z","iopub.status.idle":"2025-04-17T08:22:55.916133Z","shell.execute_reply.started":"2025-04-17T08:22:55.908116Z","shell.execute_reply":"2025-04-17T08:22:55.914917Z"}},"outputs":[{"name":"stdout","text":"Tokenized Adlam Text: {'input_ids': [128000, 135065, 220, 141096, 6, 134527, 172, 252, 98, 228, 151709, 172, 252, 98, 227, 220, 136933, 220, 140572, 220, 139655, 220, 140572, 220, 131606, 220, 150108, 133775, 172, 252, 98, 226, 141096, 220, 139015, 172, 252, 98, 227, 138603, 13, 129421, 220, 139548, 172, 252, 98, 226, 149158, 220, 156422, 172, 252, 98, 227, 144009, 220, 154500, 172, 252, 98, 227, 144303, 11, 220, 133775, 172, 252, 98, 226, 156422, 172, 252, 98, 227, 149158, 220, 148538, 172, 252, 98, 227, 220, 135708, 172, 252, 98, 227, 143486, 172, 252, 98, 227, 11, 220, 144591, 220, 135708, 172, 252, 98, 227, 143486, 172, 252, 98, 227, 13], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\nTokenized LLaMA Text: {'input_ids': [128000, 36, 342, 72, 75, 64, 220, 77, 67, 2176, 77, 11, 294, 64, 64, 3423, 78, 75, 220, 82, 84, 74, 64, 294, 68, 6194, 78, 220, 2689, 220, 68, 220, 77, 64, 14736, 68, 296, 84, 76, 326, 68, 19239, 84, 220, 72, 77, 64, 220, 88, 64, 4171, 64, 220, 68, 294, 84, 84, 133, 241, 72, 220, 972, 11, 220, 72, 77, 64, 220, 77, 73, 84, 133, 241, 133, 241, 72, 77, 2176, 220, 68, 503, 72, 76, 133, 245, 72, 282, 84, 75, 133, 241, 68, 503, 72, 76, 133, 245, 72, 326, 68, 82, 220, 64, 82, 64, 76, 64, 64, 77, 597, 2689, 67, 68, 13], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n","output_type":"stream"}],"execution_count":113},{"cell_type":"code","source":"# Decode the tokens back into text\ndecoded_adlam = llama_tokenizer.decode(tokenized_adlam['input_ids'])\nprint(\"Decoded Adlam Text:\", decoded_adlam)\n\ndecoded_llama = llama_tokenizer.decode(tokenized_llama['input_ids'])\nprint(\"Decoded LLaMA Text:\", decoded_llama)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-17T08:23:01.619657Z","iopub.execute_input":"2025-04-17T08:23:01.620096Z","iopub.status.idle":"2025-04-17T08:23:01.628005Z","shell.execute_reply.started":"2025-04-17T08:23:01.620067Z","shell.execute_reply":"2025-04-17T08:23:01.626668Z"}},"outputs":[{"name":"stdout","text":"Decoded Adlam Text: <|begin_of_text|>ğ¤¢ ğ¤²'ğ¤¶ğ¤µğ¤©ğ¥†ğ¤­ğ¤²ğ¤«ğ¥… ğ¤« ğ¤¶ğ¤­ğ¤¥ğ¤¯ğ¤­ ğ¤¬ğ¤µğ¤¤ğ¤©ğ¤« ğ¤¶ğ¤­ğ¤¥ğ¤¯ğ¤­ ğ¤¤ğ¤«ğ¤§ ğ¤¢ğ¤§ğ¤¢ğ¤¥ğ¤¢ğ¥„ğ¤² ğ¤³ğ¤®ğ¥…ğ¤£ğ¤«.ğ¤‘ğ¤®ğ¤²ğ¤® ğ¤´ğ¤®ğ¤²ğ¤¼ğ¤¢ğ¥„ğ¤¶ğ¤­ ğ¤´ğ¤®ğ¥…ğ¤ªğ¤® ğ¤ºğ¤®ğ¥…ğ¤¼ğ¤®, ğ¤¥ğ¤¢ğ¥„ğ¤´ğ¤®ğ¥…ğ¤¶ğ¤­ ğ¤¯ğ¤­ğ¥… ğ¤©ğ¤µğ¥…ğ¤©ğ¤¼ğ¤­ğ¥…, ğ¤¸ğ¤µğ¤¯ğ¤® ğ¤©ğ¤µğ¥…ğ¤©ğ¤¼ğ¤­ğ¥….\nDecoded LLaMA Text: <|begin_of_text|>E gila ndeen, daartol suka debbo oo e nagge mum lewru ina yahra e duuÉ“i 18, ina njuÉ“É“inee e jimÉ—i fulÉ“e jimÉ—i les asamaan koode.\n","output_type":"stream"}],"execution_count":114},{"cell_type":"code","source":"# Encoding the Adlam text\nencoded_adlam = llama_tokenizer.encode(sample_adlam_text)\nprint(\"Encoded Adlam Text:\", encoded_adlam)\n\n# Encoding the LLaMA text\nencoded_llama = llama_tokenizer.encode(sample_llama_text)\nprint(\"Encoded LLaMA Text:\", encoded_llama)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-17T08:24:06.718347Z","iopub.execute_input":"2025-04-17T08:24:06.718692Z","iopub.status.idle":"2025-04-17T08:24:06.726172Z","shell.execute_reply.started":"2025-04-17T08:24:06.718671Z","shell.execute_reply":"2025-04-17T08:24:06.724946Z"}},"outputs":[{"name":"stdout","text":"Encoded Adlam Text: [128000, 135065, 220, 141096, 6, 134527, 172, 252, 98, 228, 151709, 172, 252, 98, 227, 220, 136933, 220, 140572, 220, 139655, 220, 140572, 220, 131606, 220, 150108, 133775, 172, 252, 98, 226, 141096, 220, 139015, 172, 252, 98, 227, 138603, 13, 129421, 220, 139548, 172, 252, 98, 226, 149158, 220, 156422, 172, 252, 98, 227, 144009, 220, 154500, 172, 252, 98, 227, 144303, 11, 220, 133775, 172, 252, 98, 226, 156422, 172, 252, 98, 227, 149158, 220, 148538, 172, 252, 98, 227, 220, 135708, 172, 252, 98, 227, 143486, 172, 252, 98, 227, 11, 220, 144591, 220, 135708, 172, 252, 98, 227, 143486, 172, 252, 98, 227, 13]\nEncoded LLaMA Text: [128000, 36, 342, 72, 75, 64, 220, 77, 67, 2176, 77, 11, 294, 64, 64, 3423, 78, 75, 220, 82, 84, 74, 64, 294, 68, 6194, 78, 220, 2689, 220, 68, 220, 77, 64, 14736, 68, 296, 84, 76, 326, 68, 19239, 84, 220, 72, 77, 64, 220, 88, 64, 4171, 64, 220, 68, 294, 84, 84, 133, 241, 72, 220, 972, 11, 220, 72, 77, 64, 220, 77, 73, 84, 133, 241, 133, 241, 72, 77, 2176, 220, 68, 503, 72, 76, 133, 245, 72, 282, 84, 75, 133, 241, 68, 503, 72, 76, 133, 245, 72, 326, 68, 82, 220, 64, 82, 64, 76, 64, 64, 77, 597, 2689, 67, 68, 13]\n","output_type":"stream"}],"execution_count":115},{"cell_type":"code","source":"# Decoding the encoded Adlam text\ndecoded_adlam = llama_tokenizer.decode(encoded_adlam)\nprint(\"Decoded Adlam Text:\", decoded_adlam)\n\n# Decoding the encoded LLaMA text\ndecoded_llama = llama_tokenizer.decode(encoded_llama)\nprint(\"Decoded LLaMA Text:\", decoded_llama)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-17T08:24:21.885706Z","iopub.execute_input":"2025-04-17T08:24:21.886028Z","iopub.status.idle":"2025-04-17T08:24:21.892714Z","shell.execute_reply.started":"2025-04-17T08:24:21.886007Z","shell.execute_reply":"2025-04-17T08:24:21.891404Z"}},"outputs":[{"name":"stdout","text":"Decoded Adlam Text: <|begin_of_text|>ğ¤¢ ğ¤²'ğ¤¶ğ¤µğ¤©ğ¥†ğ¤­ğ¤²ğ¤«ğ¥… ğ¤« ğ¤¶ğ¤­ğ¤¥ğ¤¯ğ¤­ ğ¤¬ğ¤µğ¤¤ğ¤©ğ¤« ğ¤¶ğ¤­ğ¤¥ğ¤¯ğ¤­ ğ¤¤ğ¤«ğ¤§ ğ¤¢ğ¤§ğ¤¢ğ¤¥ğ¤¢ğ¥„ğ¤² ğ¤³ğ¤®ğ¥…ğ¤£ğ¤«.ğ¤‘ğ¤®ğ¤²ğ¤® ğ¤´ğ¤®ğ¤²ğ¤¼ğ¤¢ğ¥„ğ¤¶ğ¤­ ğ¤´ğ¤®ğ¥…ğ¤ªğ¤® ğ¤ºğ¤®ğ¥…ğ¤¼ğ¤®, ğ¤¥ğ¤¢ğ¥„ğ¤´ğ¤®ğ¥…ğ¤¶ğ¤­ ğ¤¯ğ¤­ğ¥… ğ¤©ğ¤µğ¥…ğ¤©ğ¤¼ğ¤­ğ¥…, ğ¤¸ğ¤µğ¤¯ğ¤® ğ¤©ğ¤µğ¥…ğ¤©ğ¤¼ğ¤­ğ¥….\nDecoded LLaMA Text: <|begin_of_text|>E gila ndeen, daartol suka debbo oo e nagge mum lewru ina yahra e duuÉ“i 18, ina njuÉ“É“inee e jimÉ—i fulÉ“e jimÉ—i les asamaan koode.\n","output_type":"stream"}],"execution_count":116},{"cell_type":"code","source":"from transformers import PreTrainedTokenizerFast\n\n# Load your fast tokenizer\ntokenizer = PreTrainedTokenizerFast.from_pretrained(\"/kaggle/working/extended_llama_adlam_tokenizer\")\n\n# Push it to the Hugging Face Hub\ntokenizer.push_to_hub(\"Pullo-Africa-Protagonist/ADLaM-Tokenizer\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-17T08:34:17.719142Z","iopub.execute_input":"2025-04-17T08:34:17.719524Z","iopub.status.idle":"2025-04-17T08:34:35.603336Z","shell.execute_reply.started":"2025-04-17T08:34:17.719496Z","shell.execute_reply":"2025-04-17T08:34:35.602530Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/23.6M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d3c467e34dda40b198660e2330c4fb10"}},"metadata":{}},{"execution_count":121,"output_type":"execute_result","data":{"text/plain":"CommitInfo(commit_url='https://huggingface.co/Pullo-Africa-Protagonist/ADLaM-Tokenizer/commit/a215376f920ac11c5c5858288303f0556824afa3', commit_message='Upload tokenizer', commit_description='', oid='a215376f920ac11c5c5858288303f0556824afa3', pr_url=None, repo_url=RepoUrl('https://huggingface.co/Pullo-Africa-Protagonist/ADLaM-Tokenizer', endpoint='https://huggingface.co', repo_type='model', repo_id='Pullo-Africa-Protagonist/ADLaM-Tokenizer'), pr_revision=None, pr_num=None)"},"metadata":{}}],"execution_count":121},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}